## 马尔可夫决策过程（Markov decision process，MDP）

### 状态转移矩阵
概率论讨论的是静态的随机现象【当前状态与上一状态无关，单次的状态概率情况固定】，而随机过程讨论的是随时间演变的随机现象

静态随机现象的核心是**不随时间演变，仅在单次或多次试验 / 观察中呈现随机性**，其统计规律（如概率分布）是固定不变的，**不依赖时间因素**。
- 经典随机试验类：人为设计的试验产生随机结果，无时间维度的变化。比如投骰子、抛硬币等
- 抽样与观测类：这类现象源于对固定总体的随机选取，随机性体现在 “选哪个”，而总体的统计特性不随时间改变。如产品质检、人口抽样等。
- 自然与日常观测类：这类现象是对 “某一固定状态或单次事件” 的随机观测，不存在时间上的连续演变。如彩票开奖、*特定时刻的天气状态*等。

只要一个系统存在不同的 “状态”，且系统会从一种状态变为另一种状态，就涉及状态转移。过程中状态之间的变换的区别在于**转移是否带有随机性**，可分为
- 确定性过程：当前状态直接决定了下一时刻的具体状态，有规则约束的必然转移。比如红绿灯🚥
- 随机过程：并非完全random，而是不同状态之间的转移变化都有一个概率，转移结果不确定，仅能通过概率描述。可以组成一个 $M^2$ 的状态转移矩阵


**随机过程**中的状态之间的变换分为：
- 马尔可夫链（如天气预测）：系统状态（晴、雨、阴）的转移仅依赖当前状态，且转移概率固定（如 “今天晴→明天雨” 的概率为 20%），结果是随机的。
- 股票涨跌模型：股票状态（涨、跌、平）的转移概率基于历史数据统计，下一个状态无法确定，只能用概率表示。
- 排队系统：顾客状态（等待、服务、离开）的转移受服务台效率影响，转移时机和顺序存在随机性。

$$
\begin{bmatrix}
P(s_1|s_1) & P(s_2|s_1) & ... & P(s_n|s_1) \\
P(s_1|s_2) & P(s_2|s_2) & ... & P(s_n|s_2) \\
& & ... \\
P(s_1|s_n) & P(s_2|s_n) & ... & P(s_n|s_n)
\end{bmatrix}
$$

▶️当且仅当某时刻的状态只取决于上一时刻的状态时，该随机过程具有**马尔可夫**性质、为**马尔科夫过程**，即
$P(s_{t+1}|s_t) = P(s_{t+1}|s_1,...,s_t)$。【不等价于与历史无关，因为 $s_t$ 也包含了更值钱的状态信息】


### 马尔科夫过程（Markov process，MP）

### 价值函数
强化学习的概念：基于智能体在复杂、不确定的环境中最大化其能获得的奖励，从而达到自主决策的目的。

目标：最大化智能体策略在和动态环境交互过程中的价值，而策略的价值可以等价转换成奖励函数的期望，即最大化累计下来的奖励期望

和监督学习的区别
- 监督学习有标签，目标是找到一个最优的模型函数，能最小化预测误差
- 监督学习每一步都能即刻反馈给算法，而强化学习的结果反馈有延时，走了很多步之后才意识到之前某步的选择的好坏
- 监督学习的每次输入数据都是独立的，而强化学习是序贯决策，算法做出一个行为，也就影响了下一次的输入

组成部分
- agent，智能体
- action，动作
- environment，环境，提供reward反馈
- reward，奖励，在明确目标的情况下，接近目标则奖，远离目标则惩，最终达到收益/奖励最大化
- state， $s$ ，环境的状态

学习过程：依据策略执行动作-感知状态-得到奖励，循环进行。

如何改变的参数？
- 基于价值函数的方法【不改变参数】，适合离散环境，比如围棋。
- 基于策略的方法：对当前已经搜索到的策略进行估值，根据估值进行策略改进，重复至策略收敛。适合连续动作场景。


  
